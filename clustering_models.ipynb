{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e5398d",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Clustering Models](#Clustering-Models)\n",
    "    - [KMeans](#KMeans)\n",
    "    - [Agglomerative Clustering](#Agglomerative-Clustering)\n",
    "    - [DBSCAN](#DBSCAN)\n",
    "    - [BIRCH](#BIRCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821941af",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e831b",
   "metadata": {},
   "source": [
    "To identify clusters of countries and display them in a map, we can use clustering. Here is a general outline of our steps:\n",
    "\n",
    "1. Choose a clustering algorithm: K-means clustering, hierarchical clustering, DBSCAN, etc. \n",
    "2. Cluster the data: group countries based on their metrics.\n",
    "3. Visualize the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d688b",
   "metadata": {},
   "source": [
    "# Clustering Models\n",
    "\n",
    "Using the notebook that Boris provided for week 5, we can discover the huge variety of different clustering techniques. The core idea of them is well depicted in the image below.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png' width=\"800\">\n",
    "<center/>\n",
    "\n",
    "Source: [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472414e1",
   "metadata": {},
   "source": [
    "We do not want to work with all of these models provided (sorry). However, we can roughly split different approaches and algorithms to perform clustering tasks into three sub-categories:\n",
    "- Partition-based clustering: E.g. k-means, k-median\n",
    "- Hierarchical clustering: E.g. Agglomerative, Divisive\n",
    "- Density-based clustering: E.g. DBSCAN\n",
    "\n",
    "Below we provide summaries for the techniques we will apply in our project later on:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016e88",
   "metadata": {},
   "source": [
    "- **K-Means**: This algorithm aims to partition n observations into k clusters, where each observation belongs to the cluster with the nearest mean.\n",
    "- **Hierarchical Clustering**: This algorithm builds a hierarchy of clusters either top-down (divisive) or bottom-up (agglomerative) by recursively merging or splitting clusters.\n",
    "- **DBSCAN**: This algorithm groups together points that are closely packed together (dense regions) and marks outliers that lie alone in low-density areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf7ec8",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "[KMeans model](https://scikit-learn.org/stable/modules/clustering.html#k-means) clusters data by trying to separate samples in $n$ groups of equal variance, minimizing inertia (or within-cluster sum-of-square). This algorithm requires the number of clusters to be specified. \n",
    "\n",
    "The k-means algorithm divides a set into disjoint clusters, each described by the mean (“centroids”) of the samples in the cluster. The K-means algorithm aims to choose centroids that minimise the inertia:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n}min_{\\mu_j \\in C}(||x_i-\\mu_j||^2)\n",
    "$$ where\n",
    "\n",
    "- $n$ is the number of data points,\n",
    "- $C$ is the set of clusters,\n",
    "- $\\mu_j$ is the centroid of cluster $j$,\n",
    "- $||x_i - \\mu_j||^2$ is the squared Euclidean distance between data point $i$ and the centroid $\\mu_j$.\n",
    "\n",
    "Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "\n",
    "1. Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "2. Inertia is not a normalized metric. We just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "Source: [KMeans Documentation](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bfa82",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "Or [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering). It builds nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.\n",
    "\n",
    "The [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) object performs a hierarchical clustering using a **bottom up approach**: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n",
    "\n",
    "- `Ward` *minimizes the sum of squared differences within all clusters*. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "- `Maximum` or `complete` minimizes *the maximum distance* between observations of pairs of clusters.\n",
    "- `Average` minimizes *the average of the distances* between all observations of pairs of clusters.\n",
    "- `Single` minimizes *the distance between the closest observations* of pairs of clusters.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png' width=\"400\">\n",
    "<center/>\n",
    "\n",
    "The [FeatureAgglomeration](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration) uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool.\n",
    "\n",
    "Source: [Agglomerative clustering documentation](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16aa47f",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "DBSCAN stands for *density-based spatial clustering of applications with noise*. It is able to find arbitrary shaped clusters and clusters with outliers. The main idea behind DBSCAN is that a point belongs to a cluster if it is close to many points from that cluster.\n",
    "\n",
    "The [DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan) algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). \n",
    "\n",
    "There are two parameters to the algorithm, `min_samples` and `eps`, which define formally what we mean when we say dense. Higher `min_samples` or lower `eps` indicate higher density necessary to form a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least `eps` in distance from any core sample, is considered an outlier by the algorithm.\n",
    "\n",
    "While the parameter `min_samples` primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter `eps` is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster.\n",
    "\n",
    "In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_002.png' width=\"400\">\n",
    "<center/>   \n",
    "    \n",
    "Source: [DBSCAN documentation](https://scikit-learn.org/stable/modules/clustering.html#dbscan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
