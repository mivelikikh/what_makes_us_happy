{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e5398d",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Clustering Models](#Clustering-Models)\n",
    "    - [KMeans](#KMeans)\n",
    "    - [Agglomerative Clustering](#Agglomerative-Clustering)\n",
    "    - [DBSCAN](#DBSCAN)\n",
    "    - [BIRCH](#BIRCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821941af",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e831b",
   "metadata": {},
   "source": [
    "To identify clusters of countries and display them in a map, we can use clustering. Here is a general outline of our steps:\n",
    "\n",
    "1. Choose a clustering algorithm: K-means clustering, hierarchical clustering, DBSCAN, etc. \n",
    "2. Cluster the data: group countries based on their metrics.\n",
    "3. Visualize the clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21d688b",
   "metadata": {},
   "source": [
    "# Clustering Models\n",
    "\n",
    "Using the notebook that Boris provided for week 5, we can discover the huge variety of different clustering techniques. The core idea of them is well depicted in the image below.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png' width=\"800\">\n",
    "<center/>\n",
    "\n",
    "Source: [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472414e1",
   "metadata": {},
   "source": [
    "We do not want to work with all of them (sorry). However, based on the fact how the data can be clustered, we want to apply different approaches to see how different models' performance may vary. So, below we provide summaries for all techniques we will apply in our project later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016e88",
   "metadata": {},
   "source": [
    "- K-Means: This algorithm aims to partition n observations into k clusters, where each observation belongs to the cluster with the nearest mean.\n",
    "- Hierarchical Clustering: This algorithm builds a hierarchy of clusters either top-down (divisive) or bottom-up (agglomerative) by recursively merging or splitting clusters.\n",
    "- DBSCAN: This algorithm groups together points that are closely packed together (dense regions) and marks outliers that lie alone in low-density areas.\n",
    "- Gaussian Mixture Model: This algorithm models the distribution of data as a mixture of Gaussian distributions and attempts to fit a set of k Gaussian distributions to the data.\n",
    "- Mean Shift: This algorithm identifies centroids (modes) of density function and iteratively shifts the centroids towards higher density areas.\n",
    "- Agglomerative Clustering: This algorithm starts with each point in its own cluster, then repeatedly merges the two closest clusters until only one cluster is left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf7ec8",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "[KMeans model](https://scikit-learn.org/stable/modules/clustering.html#k-means) clusters data by trying to separate samples in $n$ groups of equal variance, minimizing inertia (or within-cluster sum-of-square). This algorithm requires the number of clusters to be specified. \n",
    "\n",
    "The k-means algorithm divides a set into disjoint clusters, each described by the mean (“centroids”) of the samples in the cluster. The K-means algorithm aims to choose centroids that minimise the inertia:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n}min_{\\mu_j \\in C}(||x_i-\\mu_j||^2)\n",
    "$$ where\n",
    "\n",
    "- $n$ is the number of data points,\n",
    "- $C$ is the set of clusters,\n",
    "- $\\mu_j$ is the centroid of cluster $j$,\n",
    "- $||x_i - \\mu_j||^2$ is the squared Euclidean distance between data point $i$ and the centroid $\\mu_j$.\n",
    "\n",
    "Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "\n",
    "1. Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "2. Inertia is not a normalized metric. We just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "Source: [KMeans Documentation](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3bfa82",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "Or [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering). It builds nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.\n",
    "\n",
    "The [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) object performs a hierarchical clustering using a **bottom up approach**: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n",
    "\n",
    "- `Ward` *minimizes the sum of squared differences within all clusters*. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "- `Maximum` or `complete` minimizes *the maximum distance* between observations of pairs of clusters.\n",
    "- `Average` minimizes *the average of the distances* between all observations of pairs of clusters.\n",
    "- `Single` minimizes *the distance between the closest observations* of pairs of clusters.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png' width=\"400\">\n",
    "<center/>\n",
    "\n",
    "The [FeatureAgglomeration](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration) uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool.\n",
    "\n",
    "Source: [Agglomerative clustering documentation](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16aa47f",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "The [DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan) algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). \n",
    "\n",
    "There are two parameters to the algorithm, `min_samples` and `eps`, which define formally what we mean when we say dense. Higher `min_samples` or lower `eps` indicate higher density necessary to form a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least `eps` in distance from any core sample, is considered an outlier by the algorithm.\n",
    "\n",
    "While the parameter `min_samples` primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter `eps` is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster.\n",
    "\n",
    "In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_002.png' width=\"400\">\n",
    "<center/>   \n",
    "    \n",
    "Source: [DBSCAN documentation](https://scikit-learn.org/stable/modules/clustering.html#dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994d9d1",
   "metadata": {},
   "source": [
    "This method can be especially useful in a dataset with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463aa94c",
   "metadata": {},
   "source": [
    "## BIRCH\n",
    "\n",
    "[BIRCH](https://scikit-learn.org/stable/modules/clustering.html#birch) builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\n",
    "\n",
    "The BIRCH algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.\n",
    "\n",
    "This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by `n_clusters`. If `n_clusters` is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.\n",
    "\n",
    "As a rule of thumb `n_features` should be less than twenty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
